#!/usr/bin/env python3

import os
import re
import time
import json
import sys
import math
import unicodedata
import urllib.parse
import urllib.robotparser
import urllib.request
from collections import defaultdict, deque
from typing import List, Dict, Any, Tuple, Set

import tldextract
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

try:
    from openai import OpenAI
except Exception:
    OpenAI = None  # handled below

# Load Key.env first (if present), then fallback to .env.
# If python-dotenv isn't installed, do a simple fallback parser.
try:
    from dotenv import load_dotenv
    load_dotenv("Key.env")
    load_dotenv()
except Exception:
    # Lightweight fallback
    def _load_kv_file(path: str):
        try:
            if os.path.exists(path):
                with open(path, "r", encoding="utf-8") as f:
                    for line in f:
                        line = line.strip()
                        if not line or line.startswith("#"):
                            continue
                        if "=" in line:
                            k, v = line.split("=", 1)
                            k = k.strip(); v = v.strip().strip('"').strip("'")
                            if k and v and k not in os.environ:
                                os.environ[k] = v
        except Exception:
            pass
    _load_kv_file("Key.env")
    _load_kv_file(".env")

# =====================
# CONFIG
# =====================
EXCEL_PATH = "/Users/patrickr/Documents/VS Code/Data enrichement.xlsx"  # input workbook (column A = websites)
INPUT_SHEET_NAME = "Analyse"  # explicit first sheet name
OUTPUT_SHEET_NAME = "Enriched Sheet"  # output sheet name (existing template)

HEADLESS = True
IGNORE_ROBOTS = True  # set True during testing to avoid robots.txt skips; set False to respect robots
NAV_TIMEOUT_MS = 25000 # max time per page navigation/load
PAGE_TIMEOUT_S = 60 # max time per page load/process
DOMAIN_TIMEOUT_S = 240 # max time per domain/site
STRICT_DOMAIN_ONLY = False  # TRUE = only keep emails that match the domain
MAX_PERSON_LINKS = 20  # max person-relevant links to follow per site
VERBOSE = True  # set True for detailed logs during testing
MAX_ROWS_TO_PROCESS = 0  # limit number of Analyse rows processed (None or 0 = unlimited)
POST_COMPANY_SLEEP_S = 5  # wait between companies to reduce API pressure during testing
LLM_MAX_PAGES = 2  # limit number of pages to send to LLM per site to reduce calls (0 = unlimited)

# LLM config
MODEL_NAME = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
MAX_LLM_CALLS_PER_MIN = 30  # simple throttle
MAX_PAGE_TEXT_CHARS = 12000  # truncate per-page text sent to LLM

TARGET_PERSON_KEYWORDS = [
    # English
    "contact", "imprint", "legal", "privacy", "about", "team", "careers", "people",
    # German
    "impressum", "anbieterkennzeichnung", "datenschutz", "ueber uns", "über uns", "kontakt", "team", "karriere", "wir",
    # Other EU languages often used
    "mentions legales", "mentions légales", "aviso legal", "note legali", "contatti", "empresa", "quienes somos",
]

EMAIL_RE = re.compile(r"([a-zA-Z0-9._%+\-]+)\s*@\s*([a-zA-Z0-9.\-]+\.[a-zA-Z]{2,})", re.IGNORECASE)
MAILTO_RE = re.compile(r"^mailto:(.+)", re.IGNORECASE)

# =====================
# HELPERS
# =====================

def normalize_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def strip_accents(s: str) -> str:
    return ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c)) if s else s


def keyword_hit(href: str, text: str) -> bool:
    hay = (href or "") + " " + (text or "")
    hay = strip_accents(hay).lower()
    return any(strip_accents(kw).lower() in hay for kw in TARGET_PERSON_KEYWORDS)

def _domain_core_from_url(url: str) -> str:
    try:
        ext = tldextract.extract(url)
        return re.sub(r'[^a-z0-9]', '', (ext.domain or '').lower())
    except Exception:
        return ''

def _domain_core_from_host(host: str) -> str:
    try:
        ext = tldextract.extract('http://' + (host or ''))
        return re.sub(r'[^a-z0-9]', '', (ext.domain or '').lower())
    except Exception:
        return ''


def same_company_domain(site_url: str, email_domain: str) -> bool:
    # softened match: compare normalized “core” domain (no hyphens/underscores/etc.)
    site_core = _domain_core_from_url(site_url)
    email_core = _domain_core_from_host(email_domain)
    return bool(site_core and email_core and site_core == email_core)

def email_allowed(site_url: str, email_addr: str) -> bool:
    if not email_addr or '@' not in email_addr:
        return False
    edomain = email_addr.split('@')[-1].lower()
    if STRICT_DOMAIN_ONLY:
        return same_company_domain(site_url, edomain)
    return True

def html_preclean(html: str) -> str:
    if not html:
        return ""
    s = html
    s = re.sub(r"<style[^>]*>[\s\S]*?</style>", " ", s, flags=re.IGNORECASE)
    s = re.sub(r"<script[^>]*>[\s\S]*?</script>", " ", s, flags=re.IGNORECASE)
    s = re.sub(r"&#64;|&commat;|\(\s*at\s*\)|\[\s*at\s*\]|\{\s*at\s*\}", "@", s, flags=re.IGNORECASE)
    s = re.sub(r"\(\s*dot\s*\)|\[\s*dot\s*\]|\{\s*dot\s*\}", ".", s, flags=re.IGNORECASE)
    s = s.replace('>', ' ').replace('<', ' ').replace('&nbsp;', ' ').replace('&amp;', '&')
    return re.sub(r"\s+", " ", s).strip()


def clean_email_candidate(s: str):
    if not s:
        return None
    s = re.sub(r"[\(\[\{]\s*at\s*[\)\]\}]", "@", s, flags=re.IGNORECASE)
    s = re.sub(r"[\(\[\{]\s*dot\s*[\)\]\}]", ".", s, flags=re.IGNORECASE)
    s = re.sub(r"\bat\b", "@", s, flags=re.IGNORECASE)
    s = re.sub(r"\bdot\b", ".", s, flags=re.IGNORECASE)
    s = s.replace(">", " ").replace("<", " ").replace("&nbsp;", " ").replace("&amp;", "&")
    s = re.sub(r"\s+", " ", s).strip()
    s = re.split(r"[ \t\n\r;,\?]", s)[0]
    m = EMAIL_RE.search(s)
    return m.group(0).lower() if m else None


def extract_emails_from_text(text: str) -> Set[str]:
    emails = set()
    for m in EMAIL_RE.finditer(text):
        emails.add((m.group(1) + '@' + m.group(2)).lower())
    for chunk in re.findall(r"[\w.\-\[\]\(\)\{@\s]+(?:at|@|&#64;|&commat;)[\w.\-]+\.\w{2,}", text, flags=re.IGNORECASE):
        e = clean_email_candidate(chunk)
        if e:
            emails.add(e)
    return emails


def load_robots(base_url):
    rp = urllib.robotparser.RobotFileParser()
    robots_url = urllib.parse.urljoin(
        f"{urllib.parse.urlparse(base_url).scheme}://{urllib.parse.urlparse(base_url).netloc}",
        "/robots.txt",
    )
    try:
        with urllib.request.urlopen(robots_url, timeout=7) as resp:
            content = resp.read().decode("utf-8", errors="ignore")
        rp.parse(content.splitlines())
        print(f"[robots] Loaded: {robots_url}")
        return rp
    except Exception as e:
        print(f"[robots] Skip robots (error: {type(e).__name__}) -> {robots_url}")
        return None


def allowed_by_robots(rp, url):
    return True if IGNORE_ROBOTS or not rp else rp.can_fetch("PeopleEnricher/Playwright", url)


# =====================
# Playwright helpers
# =====================

def get_rendered_html(page, url) -> Tuple[str, str]:
    page.goto(url, wait_until="domcontentloaded", timeout=NAV_TIMEOUT_MS)
    return page.content(), page.url


def discover_person_links(page, base_url) -> List[str]:
    elements = page.eval_on_selector_all(
        "a[href]",
        "els => els.map(e => ({ href: e.getAttribute('href'), text: e.innerText }))"
    )
    abs_links: List[str] = []
    for el in elements:
        href = (el.get('href') or '').strip()
        text = normalize_spaces(el.get('text'))
        if not href:
            continue
        low = href.lower()
        if low.startswith('javascript:'):
            continue
        if low.startswith('mailto:'):
            continue
        try:
            full = urllib.parse.urljoin(base_url, href.split('#', 1)[0])
        except Exception:
            continue
        if keyword_hit(full, text):
            abs_links.append(full)
    # de-dup preserve order
    seen = set()
    kept = []
    for l in abs_links:
        if l not in seen:
            kept.append(l)
            seen.add(l)
    return kept[:MAX_PERSON_LINKS]


def prioritize_person_links(links: List[str]) -> List[str]:
    # Simple keyword-based scoring; higher is better
    weights = [
        (r'impressum|imprint|anbieterkennzeichnung', 100),
        (r'kontakt|contact', 80),
        (r'team|ueber-uns|über-uns|about', 70),
        (r'privacy|datenschutz', 30),
        (r'legal', 25),
    ]
    def score(url: str) -> int:
        s = url.lower()
        total = 0
        for pat, w in weights:
            if re.search(pat, s):
                total += w
        return total
    return sorted(links, key=score, reverse=True)


def extract_mailto_emails_from_dom(page, site_url: str) -> Set[str]:
    items = page.eval_on_selector_all('a[href^="mailto:" ]', "els => els.map(e => e.getAttribute('href'))")
    out = set()
    for href in items or []:
        try:
            cand = (href or '').split(':', 1)[1].split('?', 1)[0]
            e = clean_email_candidate(cand)
            if e and email_allowed(site_url, e):
                out.add(e.lower())
        except Exception:
            continue
    return out

# =====================
# LLM client with simple rate limiter and retries
# =====================
class LLMRateLimiter:
    def __init__(self, max_calls_per_min: int):
        self.max_calls = max(1, int(max_calls_per_min))
        self.calls = deque()

    def wait_for_slot(self):
        now = time.time()
        while self.calls and now - self.calls[0] > 60.0:
            self.calls.popleft()
        if len(self.calls) >= self.max_calls:
            sleep_s = 60.0 - (now - self.calls[0]) + 0.01
            sleep_s = max(0.5, min(30.0, sleep_s))
            if VERBOSE:
                print(f"[llm] Throttle: sleeping {sleep_s:.1f}s")
            time.sleep(sleep_s)
        self.calls.append(time.time())


class LLMClient:
    def __init__(self, model: str, rate_limiter: LLMRateLimiter):
        if OpenAI is None:
            raise RuntimeError("OpenAI SDK not installed. Run: pip install openai")
        if not os.getenv("OPENAI_API_KEY"):
            raise RuntimeError("OPENAI_API_KEY not set in environment.")
        self.client = OpenAI()
        self.model = model
        self.rate = rate_limiter

    @retry(stop=stop_after_attempt(4), wait=wait_exponential(multiplier=1, min=1, max=12),
           retry=retry_if_exception_type(Exception))
    def chat_json(self, system_prompt: str, user_prompt: str) -> Dict[str, Any]:
        self.rate.wait_for_slot()
        resp = self.client.chat.completions.create(
            model=self.model,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1,
        )
        content = resp.choices[0].message.content or "{}"
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            # try to salvage JSON
            cleaned = content.strip()
            start = cleaned.find('{')
            end = cleaned.rfind('}')
            if start != -1 and end != -1 and end > start:
                return json.loads(cleaned[start:end+1])
            raise


@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=1, max=8),
       retry=retry_if_exception_type(Exception))
def llm_chat_text(llm: LLMClient, system_prompt: str, user_prompt: str) -> str:
    # Simple text response without JSON enforcement; used as a robust fallback
    llm.rate.wait_for_slot()
    resp = llm.client.chat.completions.create(
        model=llm.model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,
    )
    return (resp.choices[0].message.content or "").strip()

# =====================
# LLM prompts
# =====================
PERSON_SYS = (
    "Du bist ein präziser Extraktionsassistent. Extrahiere PERSONEN aus Webseiteninhalten.\n"
    "Regeln: \n"
    "- Gib ausschließlich Informationen zurück, die im Text vorhanden sind. Keine Halluzinationen.\n"
    "- E-Mail-Adressen nur aus dem bereitgestellten Text/Linkliste. Keine Ableitungen oder Vermutungen.\n"
    "- Vornamen und Nachnamen als separate Felder.\n"
    "- Rolle/Funktion der Person im Unternehmen als 'role' (z. B. Verantwortlicher, Geschäftsführer, Inhaber, Assistenz), wenn erkennbar.\n"
    "- Geschlecht: male | female | unknown (nur wenn klar erkennbar).\n"
    "- Generiere das Feld 'salutation' NUR nach diesen Regeln:\n"
    "  * Wenn gender = female und last_name vorhanden: 'Sehr geehrte Frau [last_name]'.\n"
    "  * Wenn gender = male und last_name vorhanden: 'Sehr geehrter Herr [last_name]'.\n"
    "  * Wenn gender = female und last_name fehlt: 'Liebe [first_name]'.\n"
    "  * Wenn gender = male und last_name fehlt: 'Lieber [first_name]'.\n"
    "  * Ansonsten: leere Zeichenkette.\n"
    "- Sprache: Deutsch.\n"
    "Format: JSON-Objekt mit Feld 'people' = Liste von Personenobjekten.\n"
)

PERSON_USER_TMPL = (
    "Website: {site_url}\n"
    "Seite: {page_url}\n"
    "Bekannte E-Mail-Adressen (auf dieser Seite gefunden): {emails}\n\n"
    "Inhalt (abgeschnitten):\n{page_text}\n\n"
    "Gib ein JSON-Objekt mit: people: [{{first_name, last_name, role, gender, email, salutation}}].\n"
    "- email nur, wenn in den bereitgestellten E-Mails enthalten ODER eindeutig im Text vorhanden.\n"
    "- salutation 'formal' oder 'informal' in Bezug auf diese Person.\n"
    "- Keine weiteren Felder.\n"
)

COMPANY_SYS = (
    "Du analysierst die Startseite eines Unternehmens und gibst eine knappe, sachliche Zusammenfassung (max. 25 Wörter).\n"
    "Gib ein JSON-Objekt mit GENAU einem Feld: company_summary.\n"
    "Nur aus Text ableiten, keine Vermutungen. Sprache: Deutsch.\n"
)

COMPANY_USER_TMPL = (
    "Domain: {site_url}\n"
    "Startseiten-Text (abgeschnitten):\n{homepage_text}\n\n"
    "Gib ein JSON-Objekt mit dem Feld company_summary."
)


ICEBREAKER_SYS_PLAIN = (
    "Schreibe 1–2 sehr kurze, personalisierte und DSGVO-konforme Sätze auf Deutsch.\n"
    "Regeln:\n"
    "- Verwende nur Inhalte aus dem bereitgestellten Startseitentext; keine Erfindungen.\n"
    "- Nenne NICHT den Unternehmensnamen oder Ort.\n"
    "- Ziel: Es soll so wirken, als hätten wir die Website persönlich angesehen.\n"
    "- Wenn nichts Konkretes vorhanden ist, gib eine leere Zeichenkette zurück (\"\").\n"
    "- Gib NUR den Satz bzw. die Sätze aus, kein JSON, keine Erklärungen."
)

ICEBREAKER_USER_PLAIN = (
    "Domain: {site_url}\n"
    "Startseiten-Text (abgeschnitten):\n{homepage_text}\n\n"
    "Gib NUR den Satz/die Sätze aus. Kein JSON."
)


# =====================
# Excel I/O
# =====================

def open_workbook():
    from openpyxl import load_workbook
    import os
    if not os.path.exists(EXCEL_PATH):
        raise FileNotFoundError(f"Excel file not found: {EXCEL_PATH}")
    return load_workbook(EXCEL_PATH)


def read_input_urls(wb) -> List[Tuple[str, str]]:
    ws = wb[wb.sheetnames[0]] if INPUT_SHEET_NAME is None else wb[INPUT_SHEET_NAME]
    sites: List[Tuple[str, str]] = []
    for row in ws.iter_rows(min_row=2, values_only=True):
        company = (row[0] or "").strip() if row and len(row) > 0 and row[0] else ""
        website = (row[1] or "").strip() if row and len(row) > 1 and row[1] else ""
        if not website:
            continue
        website = website if website.startswith("http") else "https://" + website
        sites.append((company, website))
    return sites


def ensure_people_sheet(wb):
    # Use existing template only; do not create new sheets or headers
    if OUTPUT_SHEET_NAME in wb.sheetnames:
        return wb[OUTPUT_SHEET_NAME]
    raise ValueError(f"Output sheet not found: {OUTPUT_SHEET_NAME}")


def append_people_rows(ws, rows: List[List[Any]]):
    for r in rows:
        ws.append(r)

# Header-aware append so we always write to the next truly free row (ignoring Excel's used-range artifacts)

def get_header_map(ws) -> Dict[str, int]:
    headers = {}
    for col_idx, cell in enumerate(ws[1], start=1):
        key = str(cell.value or "").strip()
        if key:
            headers[key] = col_idx
    return headers


def _row_has_any_value(ws, row_idx: int, cols: List[int]) -> bool:
    for c in cols:
        v = ws.cell(row=row_idx, column=c).value
        if v is not None and str(v).strip() != "":
            return True
    return False


def find_next_value_row(ws, candidate_cols: List[int]) -> int:
    last = 1
    maxr = ws.max_row or 1
    for r in range(maxr, 1, -1):
        if _row_has_any_value(ws, r, candidate_cols):
            last = r
            break
    return last + 1


def append_rows_by_headers(ws, header_map: Dict[str, int], row_dicts: List[Dict[str, Any]]):
    # Determine columns that define a used row (prefer Company Name + Website URL)
    key_cols = []
    for name in ("Company Name", "Website URL"):
        if name in header_map:
            key_cols.append(header_map[name])
    if not key_cols:
        key_cols = list(header_map.values())

    for row_data in row_dicts:
        target_row = find_next_value_row(ws, key_cols)
        for k, v in row_data.items():
            if k in header_map:
                ws.cell(row=target_row, column=header_map[k], value=v)


def excel_file_locked(path: str) -> bool:
    """Return True only if we have strong evidence the workbook is open elsewhere.

    Rules:
    - If advisory lock (flock) is available and succeeds, treat as NOT locked, even if a ~$
      lock file exists (likely stale). Attempt to remove the lock file.
    - If advisory lock fails with BlockingIOError, treat as locked.
    - If flock is not available, fall back to lock-file presence, but consider files older
      than 5 minutes as stale and remove them.
    - Environment override: set EXCEL_IGNORE_LOCKS=1 to skip this check entirely.
    """
    if os.getenv("EXCEL_IGNORE_LOCKS") == "1":
        if VERBOSE:
            print("[excel] EXCEL_IGNORE_LOCKS=1 — skipping lock checks.")
        return False

    try:
        dirn, base = os.path.split(path)
        lock_path = os.path.join(dirn, "~$" + base)
        has_lock_file = os.path.exists(lock_path)

        # Try advisory lock first (best signal on POSIX)
        try:
            import fcntl  # only on POSIX
            with open(path, "rb") as f:
                try:
                    fcntl.flock(f.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
                    # Lock acquired => not locked by another process.
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)
                    if has_lock_file:
                        # If we can lock, the lock file is stale — try to remove it.
                        try:
                            os.remove(lock_path)
                            if VERBOSE:
                                print(f"[excel] Removed stale Excel lock file: {lock_path}")
                        except Exception:
                            pass
                    return False
                except BlockingIOError:
                    if VERBOSE:
                        print("[excel] Advisory lock in use (BlockingIOError).")
                    return True
        except Exception:
            # No flock available or failed for non-lock reasons — fall back to lock file heuristic
            pass

        # Fallback when flock isn't usable: rely on lock file recency
        if has_lock_file:
            try:
                age = time.time() - os.path.getmtime(lock_path)
            except Exception:
                age = 0
            if age > 300:  # >5 minutes => stale; try remove and continue
                try:
                    os.remove(lock_path)
                    if VERBOSE:
                        print(f"[excel] Removed stale Excel lock file (>5min): {lock_path}")
                except Exception:
                    pass
                return False
            if VERBOSE:
                print(f"[excel] Detected Excel lock file: {lock_path}")
            return True

        return False

    except Exception:
        # Any unexpected issue -> assume not locked, let later save path report real errors.
        return False


def norm_url(u: str) -> str:
    u = (u or "").strip().lower()
    try:
        p = urllib.parse.urlparse(u if u.startswith("http") else "https://" + u)
        host = p.netloc.lstrip("www.")
        return host
    except Exception:
        return u


def read_existing_websites(wb) -> Set[str]:
    existing: Set[str] = set()
    if OUTPUT_SHEET_NAME in wb.sheetnames:
        ws = wb[OUTPUT_SHEET_NAME]
        header_map = get_header_map(ws)
        w_col = header_map.get("Website URL")
        if w_col:
            for row_idx in range(2, (ws.max_row or 1) + 1):
                website = str(ws.cell(row=row_idx, column=w_col).value or "").strip()
                if website:
                    existing.add(norm_url(website))
    return existing


# =====================
# Core pipeline per site
# =====================

def extract_page_text_and_emails(html: str) -> Tuple[str, Set[str]]:
    cleaned = html_preclean(html)
    soup = BeautifulSoup(cleaned, 'html.parser')
    text = soup.get_text(" ")
    text = normalize_spaces(text)
    emails = extract_emails_from_text(text)
    return text, emails


def infer_company_name(site_url: str) -> str:
    es = tldextract.extract(site_url)
    return es.domain.capitalize() if es and es.domain else site_url


def process_site(site_url: str, company_name: str, llm: LLMClient, wb) -> int:
    # returns number of people written
    from openpyxl.utils import get_column_letter

    if VERBOSE:
        print(f"[process] Company='{(company_name or '').strip()}', URL='{site_url}'")

    rp = load_robots(site_url)



    # Ensure we can write at least a company-only row even if robots disallow
    people_ws = ensure_people_sheet(wb)
    header_map = get_header_map(people_ws)  # add this line
    comp_name = (company_name or infer_company_name(site_url)).strip()

    merged_emails: Set[str] = set() 

    # If robots disallow and we're respecting robots, still write a company-only row
    if not allowed_by_robots(rp, site_url):
        print(f"[skip] robots.txt disallows: {site_url}")
        empty_dict = {
            "Company Name": comp_name,
            "Website URL": site_url,
            "Company summary": "",
            "Icebreaker sentence": "",
        }
        append_rows_by_headers(people_ws, header_map, [empty_dict])
        wb.save(EXCEL_PATH)
        print(f"[excel] Wrote company-only row due to robots.txt for {site_url}")
        return 0

    t0 = time.monotonic()
    people_rows: List[Dict[str, Any]] = []
    homepage_text = ""
    found_total = 0
    company_summary = ""
    seen_people_keys: Set[str] = set()  # dedupe across pages

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=HEADLESS)
        context = browser.new_context(user_agent="PeopleEnricher/Playwright", ignore_https_errors=True)
        def _router(route):
            req = route.request
            if req.resource_type in ("image", "media", "font"):
                return route.abort()
            bad = ("doubleclick.net", "googletagmanager.com", "googlesyndication.com", "facebook.net", "twitter.com", "hotjar", "cloudflareinsights")
            if any(b in req.url for b in bad):
                return route.abort()
            return route.continue_()
        context.route("**/*", _router)
        context.set_default_navigation_timeout(NAV_TIMEOUT_MS)
        context.set_default_timeout(NAV_TIMEOUT_MS)
        page = context.new_page()

        # Load homepage
        try:
            html0, final0 = get_rendered_html(page, site_url)
            homepage_text, homepage_emails = extract_page_text_and_emails(html0)
            # filter and add
            homepage_emails = {e for e in homepage_emails if email_allowed(site_url, e)}
            mailtos_home = extract_mailto_emails_from_dom(page, site_url)
            merged_emails.update(homepage_emails)
            merged_emails.update(mailtos_home)
            person_links = discover_person_links(page, final0)
            person_links = prioritize_person_links(person_links)
            if VERBOSE:
                print(f"[discover] {site_url} -> {len(person_links)} person pages; homepage emails: {len(homepage_emails|mailtos_home)}")
            # Phase 1: pre-scan target pages for emails only (no LLM)
            target_pages = person_links or [site_url]
            if VERBOSE and target_pages:
                print(f"[emails] {site_url}: scanning {len(target_pages)} page(s) for emails")
            for url in target_pages:
                if not allowed_by_robots(rp, url):
                    if VERBOSE:
                        print(f"[skip] robots.txt (subpage): {url}")
                    continue
                try:
                    html_tmp, final_tmp = get_rendered_html(page, url)
                    page_text_tmp, page_emails_tmp = extract_page_text_and_emails(html_tmp)
                    page_emails_tmp = {e for e in page_emails_tmp if email_allowed(site_url, e)}
                    mailtos_tmp = extract_mailto_emails_from_dom(page, site_url)
                    merged_emails.update(page_emails_tmp)
                    merged_emails.update(mailtos_tmp)
                except Exception:
                    continue
            if not merged_emails:
                empty_dict = {"Company Name": comp_name, "Website URL": site_url, "Company summary": "", "Icebreaker sentence": ""}
                append_rows_by_headers(people_ws, header_map, [empty_dict])
                try:
                    wb.save(EXCEL_PATH)
                except PermissionError:
                    print(f"[excel] Save failed: file appears open/locked: {EXCEL_PATH}")
                    sys.exit(1)
                print(f"[excel] No emails found. Wrote company-only row for {site_url}")
                return 0
        except PWTimeout:
            print(f"[timeout] Initial load timed out: {site_url}")
            browser.close()
            empty_dict = {"Company Name": comp_name, "Website URL": site_url, "Company summary": "", "Icebreaker sentence": ""}
            append_rows_by_headers(people_ws, header_map, [empty_dict])
            try:
                wb.save(EXCEL_PATH)
            except PermissionError:
                print(f"[excel] Save failed: file appears open/locked: {EXCEL_PATH}")
                sys.exit(1)
            print(f"[excel] Wrote company-only row after timeout for {site_url}")
            return 0
        except Exception as e:
            print(f"[error] Initial fetch: {e}")
            browser.close()
            empty_dict = {"Company Name": comp_name, "Website URL": site_url, "Company summary": "", "Icebreaker sentence": ""}
            append_rows_by_headers(people_ws, header_map, [empty_dict])
            try:
                wb.save(EXCEL_PATH)
            except PermissionError:
                print(f"[excel] Save failed: file appears open/locked: {EXCEL_PATH}")
                sys.exit(1)
            print(f"[excel] Wrote company-only row after error for {site_url}")
            return 0

        # Company analysis (summary only)
        try:
            h_text = homepage_text[:MAX_PAGE_TEXT_CHARS]
            company_payload = COMPANY_USER_TMPL.format(site_url=site_url, homepage_text=h_text)
            company_json = llm.chat_json(COMPANY_SYS, company_payload)
            if isinstance(company_json, dict):
                company_summary = normalize_spaces(str(company_json.get("company_summary", "")))
            if VERBOSE:
                print(f"[company] Summary: {company_summary[:120]}")
        except Exception as e:
            if VERBOSE:
                print(f"[warn] Company analysis LLM failed: {e}")

        # Icebreaker sentence (plain text, no JSON)
        icebreaker_sentence = ""
        try:
            raw = llm_chat_text(
                llm,
                ICEBREAKER_SYS_PLAIN,
                ICEBREAKER_USER_PLAIN.format(site_url=site_url, homepage_text=h_text),
            )
            icebreaker_sentence = normalize_spaces(raw.strip().strip('"').strip("'"))
            # sanitize meaningless tokens
            if icebreaker_sentence.lower() in {"icebreaker", "icebreakersatz"} or len(icebreaker_sentence) < 5:
                icebreaker_sentence = ""
        except Exception:
            if VERBOSE:
                print("[info] Icebreaker text fetch failed; leaving empty.")
            icebreaker_sentence = ""

        # If no person links found, also analyze homepage as fallback
        target_pages = person_links or [site_url]
        if VERBOSE and target_pages:
            print(f"[crawl] {site_url}: processing {len(target_pages)} page(s)")

        for idx, url in enumerate(target_pages, 1):
            if time.monotonic() - t0 > DOMAIN_TIMEOUT_S:
                print(f"[watchdog] Domain timeout reached for {site_url}")
                break
            if not allowed_by_robots(rp, url):
                if VERBOSE:
                    print(f"[skip] robots.txt (subpage): {url}")
                continue
            try:
                html, final = get_rendered_html(page, url)
                page_text, page_emails = extract_page_text_and_emails(html)
                if VERBOSE:
                    print(f"[page] {idx}/{len(target_pages)} {final} — emails found: {len(page_emails)}")
                # Optionally limit LLM calls to first N pages per site
                do_llm = (LLM_MAX_PAGES <= 0) or (idx <= LLM_MAX_PAGES)
                people = []
                if do_llm:
                    payload = PERSON_USER_TMPL.format(
                        site_url=site_url,
                        page_url=final,
                        emails=", ".join(sorted(page_emails)) if page_emails else "(keine)",
                        page_text=page_text[:MAX_PAGE_TEXT_CHARS]
                    )
                    out = llm.chat_json(PERSON_SYS, payload)
                    people = out.get("people", []) if isinstance(out, dict) else []
                    if not isinstance(people, list):
                        people = []
                    if VERBOSE:
                        print(f"[llm] {final}: extracted people = {len(people)}")
                for p_obj in people:
                    try:
                        fn = normalize_spaces(str(p_obj.get("first_name", "")))
                        ln = normalize_spaces(str(p_obj.get("last_name", "")))
                        if not fn or not ln:
                            continue
                        role = normalize_spaces(str(p_obj.get("role") or p_obj.get("title") or p_obj.get("position") or ""))
                        email = normalize_spaces(str(p_obj.get("email", ""))).lower()
                        if email and email not in page_emails and email not in homepage_emails:
                            email = ""
                        gender = str(p_obj.get("gender", "unknown")).lower()
                        sal = normalize_spaces(str(p_obj.get("salutation", "")))
                        if not sal:
                            # Fallback salutation rules
                            if gender == "female":
                                sal = f"Sehr geehrte Frau {ln}" if ln else (f"Liebe {fn}" if fn else "")
                            elif gender == "male":
                                sal = f"Sehr geehrter Herr {ln}" if ln else (f"Lieber {fn}" if fn else "")
                            else:
                                sal = ""
                        key = (fn.lower(), ln.lower(), email)
                        if key in seen_people_keys:
                            continue
                        seen_people_keys.add(key)
                        people_rows.append({
                            "Company Name": comp_name,
                            "Website URL": site_url,
                            "Company summary": company_summary,
                            "Job title": role,
                            "First name": fn,
                            "Last name": ln,
                            "Gender": gender,
                            "Salutation": sal,
                            "Emails": email,
                            "Icebreaker sentence": icebreaker_sentence,
                        })
                        found_total += 1
                    except Exception:
                        continue
            except PWTimeout:
                print(f"[timeout] Page load timed out: {url}")
            except Exception as e:
                print(f"[error] Fetch {url}: {e}")

        browser.close()

    # Append to Excel with before/after row counts (true last-used rows)
    try:
        # compute true last-used row using key columns
        key_cols = []
        for name in ("Company Name", "Website URL"):
            if name in header_map:
                key_cols.append(header_map[name])
        if not key_cols:
            key_cols = list(header_map.values())
        before_rows = find_next_value_row(people_ws, key_cols) - 1

        if people_rows:
            append_rows_by_headers(people_ws, header_map, people_rows)
            try:
                wb.save(EXCEL_PATH)
            except PermissionError:
                print(f"[excel] Save failed: file appears open/locked: {EXCEL_PATH}")
                sys.exit(1)
            after_rows = find_next_value_row(people_ws, key_cols) - 1
            print(f"[excel] {EXCEL_PATH} :: '{OUTPUT_SHEET_NAME}' — wrote {len(people_rows)} row(s), people={found_total}; rows before={before_rows}, after={after_rows}")
    # ... existing code for people_rows case ...
        else:
            # Even if nothing else found, write company + website row with all emails and general salutation
            empty_dict = {
                "Company Name": comp_name,
                "Website URL": site_url,
                "Company summary": company_summary,
                "Job title": "",
                "First name": "",
                "Last name": "",
                "Gender": "",
                "Salutation": "Sehr geehrte Damen und Herren" if merged_emails else "",  # generic if emails present
                "Emails": "; ".join(sorted(merged_emails)) if merged_emails else "",
                "Icebreaker sentence": icebreaker_sentence,
        }
        append_rows_by_headers(people_ws, header_map, [empty_dict])
        try:
            wb.save(EXCEL_PATH)
        except PermissionError:
            print(f"[excel] Save failed: file appears open/locked: {EXCEL_PATH}")
            sys.exit(1)
        after_rows = find_next_value_row(people_ws, key_cols) - 1
        print(f"[excel] {EXCEL_PATH} :: '{OUTPUT_SHEET_NAME}' — wrote company-only row; rows before={before_rows}, after={after_rows}")
    except PermissionError as pe:
        print(f"[excel] Save failed (is the file open/locked?): {pe}")
        sys.exit(1)

    return found_total


# =====================
# main() — bulk loop
# =====================

def main():
    # Init LLM client
    try:
        llm = LLMClient(MODEL_NAME, LLMRateLimiter(MAX_LLM_CALLS_PER_MIN))
    except Exception as e:
        print(f"[fatal] LLM init failed: {e}")
        return

    try:
        # pre-check for locked Excel file (simplified: only rely on true lock detection)
        if excel_file_locked(EXCEL_PATH):
            print(f"[excel] File appears open/locked: {EXCEL_PATH}. Please close it and re-run.")
            sys.exit(1)

        wb = open_workbook()
        sites = read_input_urls(wb)  # (company, website)
        existing = read_existing_websites(wb)
        to_process = [(c, w) for (c, w) in sites if norm_url(w) not in existing]
        if MAX_ROWS_TO_PROCESS:
            to_process = to_process[:MAX_ROWS_TO_PROCESS]
        print(f"[bulk] Loaded {len(sites)} from Analyse; already enriched: {len(existing)}; processing now: {len(to_process)}")
        total_people = 0
        for i, (company, site) in enumerate(to_process, 1):
            print(f"\n[site] ({i}/{len(to_process)}) {site} — company='{company}'")
            try:
                total_people += process_site(site, company, llm, wb)
            except Exception as ex:
                print(f"[error] Unhandled for {site}: {ex}")
            finally:
                if POST_COMPANY_SLEEP_S:
                    if VERBOSE:
                        print(f"[sleep] Waiting {POST_COMPANY_SLEEP_S}s before next company...")
                    time.sleep(POST_COMPANY_SLEEP_S)
        print(f"\n[done] Total people extracted: {total_people}")
    except Exception as ex:
        print(f"[bulk] Error: {ex}")


if __name__ == '__main__':
    main()

